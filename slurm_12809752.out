Using 1 GPUs
Files already downloaded and verified
Files already downloaded and verified
arch resnet110
dataset cifar10
Attacker is DDN
Epoch: [0][0/196]	Time 1.042 (1.042)	Data 0.629 (0.629)	Loss 4.0038 (4.0038)	Acc@1 8.984 (8.984)	Acc@5 51.562 (51.562)
Epoch: [0][10/196]	Time 0.186 (0.265)	Data 0.000 (0.059)	Loss 8.2633 (4.9153)	Acc@1 8.594 (11.648)	Acc@5 46.484 (50.888)
Epoch: [0][20/196]	Time 0.188 (0.228)	Data 0.002 (0.032)	Loss 2.4031 (3.9501)	Acc@1 10.156 (11.068)	Acc@5 53.906 (50.335)
Epoch: [0][30/196]	Time 0.187 (0.215)	Data 0.002 (0.022)	Loss 2.3156 (3.4442)	Acc@1 11.719 (11.051)	Acc@5 53.906 (51.147)
Epoch: [0][40/196]	Time 0.187 (0.208)	Data 0.002 (0.017)	Loss 2.2985 (3.1727)	Acc@1 10.547 (10.852)	Acc@5 50.000 (50.991)
Epoch: [0][50/196]	Time 0.188 (0.204)	Data 0.002 (0.014)	Loss 2.3067 (3.0037)	Acc@1 10.547 (10.800)	Acc@5 53.516 (51.256)
Epoch: [0][60/196]	Time 0.187 (0.201)	Data 0.001 (0.012)	Loss 2.2883 (2.8878)	Acc@1 17.969 (10.873)	Acc@5 51.953 (51.530)
Epoch: [0][70/196]	Time 0.188 (0.199)	Data 0.002 (0.011)	Loss 2.2907 (2.8047)	Acc@1 9.766 (10.943)	Acc@5 55.078 (51.750)
Epoch: [0][80/196]	Time 0.188 (0.198)	Data 0.002 (0.010)	Loss 2.3042 (2.7410)	Acc@1 10.156 (10.942)	Acc@5 49.219 (51.900)
Epoch: [0][90/196]	Time 0.187 (0.197)	Data 0.002 (0.009)	Loss 2.2988 (2.6926)	Acc@1 11.328 (10.843)	Acc@5 50.391 (51.850)
Epoch: [0][100/196]	Time 0.188 (0.196)	Data 0.002 (0.008)	Loss 2.2953 (2.6530)	Acc@1 9.766 (10.849)	Acc@5 57.422 (52.030)
Epoch: [0][110/196]	Time 0.187 (0.195)	Data 0.002 (0.007)	Loss 2.2977 (2.6207)	Acc@1 12.500 (10.881)	Acc@5 52.734 (51.999)
Epoch: [0][120/196]	Time 0.187 (0.195)	Data 0.002 (0.007)	Loss 2.2893 (2.5936)	Acc@1 12.109 (10.873)	Acc@5 51.172 (52.095)
Epoch: [0][130/196]	Time 0.187 (0.194)	Data 0.002 (0.007)	Loss 2.2825 (2.5707)	Acc@1 13.672 (10.863)	Acc@5 57.422 (52.219)
Epoch: [0][140/196]	Time 0.188 (0.194)	Data 0.002 (0.006)	Loss 2.2693 (2.5504)	Acc@1 16.016 (11.004)	Acc@5 62.500 (52.316)
Epoch: [0][150/196]	Time 0.188 (0.193)	Data 0.002 (0.006)	Loss 2.2888 (2.5329)	Acc@1 17.578 (11.106)	Acc@5 56.641 (52.538)
Epoch: [0][160/196]	Time 0.187 (0.193)	Data 0.002 (0.006)	Loss 2.2684 (2.5178)	Acc@1 12.891 (11.149)	Acc@5 53.906 (52.676)
Epoch: [0][170/196]	Time 0.188 (0.192)	Data 0.002 (0.006)	Loss 2.2429 (2.5037)	Acc@1 16.797 (11.175)	Acc@5 60.938 (52.814)
Epoch: [0][180/196]	Time 0.187 (0.192)	Data 0.002 (0.005)	Loss 2.2533 (2.4911)	Acc@1 14.844 (11.209)	Acc@5 55.078 (52.959)
Epoch: [0][190/196]	Time 0.188 (0.192)	Data 0.002 (0.005)	Loss 2.2684 (2.4802)	Acc@1 8.984 (11.220)	Acc@5 62.500 (53.174)
Test: [0/40]	Time 0.628 (0.628)	Data 0.572 (0.572)	Loss 2.2873 (2.2873)	Acc@1 12.891 (12.891)	Acc@5 56.250 (56.250)
Test: [10/40]	Time 0.056 (0.109)	Data 0.001 (0.053)	Loss 2.2859 (2.2753)	Acc@1 12.891 (11.435)	Acc@5 64.453 (58.665)
Test: [20/40]	Time 0.057 (0.084)	Data 0.002 (0.029)	Loss 2.2787 (2.2758)	Acc@1 11.719 (11.570)	Acc@5 57.812 (58.501)
Test: [30/40]	Time 0.057 (0.076)	Data 0.002 (0.020)	Loss 2.2689 (2.2742)	Acc@1 10.156 (12.021)	Acc@5 55.469 (58.959)
Saving at epoch:  1
/home/mp5847/env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/home/mp5847/env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Epoch: [1][0/196]	Time 0.825 (0.825)	Data 0.638 (0.638)	Loss 2.3297 (2.3297)	Acc@1 10.156 (10.156)	Acc@5 53.125 (53.125)
Epoch: [1][10/196]	Time 0.186 (0.246)	Data 0.001 (0.060)	Loss 2.2746 (2.2755)	Acc@1 13.672 (11.328)	Acc@5 52.344 (56.001)
Epoch: [1][20/196]	Time 0.187 (0.218)	Data 0.002 (0.032)	Loss 2.2715 (2.2735)	Acc@1 12.500 (11.514)	Acc@5 57.031 (57.403)
Epoch: [1][30/196]	Time 0.188 (0.208)	Data 0.002 (0.022)	Loss 2.2840 (2.2730)	Acc@1 10.938 (11.505)	Acc@5 60.156 (58.266)
Epoch: [1][40/196]	Time 0.187 (0.203)	Data 0.002 (0.017)	Loss 2.2567 (2.2687)	Acc@1 11.328 (11.709)	Acc@5 62.500 (59.413)
Epoch: [1][50/196]	Time 0.188 (0.200)	Data 0.002 (0.014)	Loss 2.2736 (2.2662)	Acc@1 14.062 (12.002)	Acc@5 65.625 (59.926)
Epoch: [1][60/196]	Time 0.188 (0.198)	Data 0.002 (0.012)	Loss 2.2568 (2.2594)	Acc@1 16.406 (12.455)	Acc@5 66.016 (60.970)
Epoch: [1][70/196]	Time 0.188 (0.197)	Data 0.002 (0.011)	Loss 2.2490 (2.2559)	Acc@1 12.891 (12.555)	Acc@5 63.281 (61.658)
Epoch: [1][80/196]	Time 0.188 (0.195)	Data 0.002 (0.010)	Loss 2.2259 (2.2554)	Acc@1 16.016 (12.659)	Acc@5 62.500 (61.675)
Epoch: [1][90/196]	Time 0.188 (0.195)	Data 0.002 (0.009)	Loss 2.2370 (2.2513)	Acc@1 10.938 (12.904)	Acc@5 64.062 (62.251)
Epoch: [1][100/196]	Time 0.188 (0.194)	Data 0.002 (0.008)	Loss 2.1207 (2.2435)	Acc@1 16.406 (13.219)	Acc@5 73.438 (63.045)
Epoch: [1][110/196]	Time 0.188 (0.193)	Data 0.002 (0.008)	Loss 2.1080 (2.2352)	Acc@1 16.797 (13.461)	Acc@5 76.953 (63.876)
Epoch: [1][120/196]	Time 0.188 (0.193)	Data 0.002 (0.007)	Loss 2.1595 (2.2260)	Acc@1 16.406 (13.682)	Acc@5 74.609 (64.708)
Epoch: [1][130/196]	Time 0.188 (0.192)	Data 0.002 (0.007)	Loss 2.1391 (2.2173)	Acc@1 19.141 (14.051)	Acc@5 70.703 (65.363)
Epoch: [1][140/196]	Time 0.188 (0.192)	Data 0.002 (0.006)	Loss 2.0944 (2.2093)	Acc@1 23.438 (14.381)	Acc@5 76.953 (65.993)
Epoch: [1][150/196]	Time 0.188 (0.192)	Data 0.002 (0.006)	Loss 2.0539 (2.2004)	Acc@1 21.484 (14.761)	Acc@5 78.125 (66.673)
Epoch: [1][160/196]	Time 0.187 (0.192)	Data 0.002 (0.006)	Loss 2.1008 (2.1933)	Acc@1 19.922 (15.120)	Acc@5 71.094 (67.234)
Epoch: [1][170/196]	Time 0.188 (0.191)	Data 0.002 (0.006)	Loss 2.0821 (2.1870)	Acc@1 17.969 (15.394)	Acc@5 76.562 (67.674)
Epoch: [1][180/196]	Time 0.188 (0.191)	Data 0.002 (0.005)	Loss 2.0254 (2.1797)	Acc@1 23.438 (15.817)	Acc@5 79.688 (68.249)
Epoch: [1][190/196]	Time 0.187 (0.191)	Data 0.002 (0.005)	Loss 2.0463 (2.1729)	Acc@1 23.047 (16.112)	Acc@5 76.953 (68.738)
Test: [0/40]	Time 0.655 (0.655)	Data 0.599 (0.599)	Loss 2.2811 (2.2811)	Acc@1 14.062 (14.062)	Acc@5 62.109 (62.109)
Test: [10/40]	Time 0.056 (0.112)	Data 0.001 (0.056)	Loss 2.3126 (2.2724)	Acc@1 12.891 (16.868)	Acc@5 61.719 (62.891)
Test: [20/40]	Time 0.057 (0.086)	Data 0.002 (0.030)	Loss 2.3258 (2.2687)	Acc@1 14.062 (16.536)	Acc@5 58.203 (63.095)
Test: [30/40]	Time 0.057 (0.077)	Data 0.002 (0.021)	Loss 2.2157 (2.2766)	Acc@1 13.672 (16.444)	Acc@5 63.281 (62.676)
Saving at epoch:  2
Epoch: [2][0/196]	Time 0.830 (0.830)	Data 0.644 (0.644)	Loss 2.0463 (2.0463)	Acc@1 20.703 (20.703)	Acc@5 77.344 (77.344)
Epoch: [2][10/196]	Time 0.186 (0.246)	Data 0.001 (0.060)	Loss 2.0392 (2.0430)	Acc@1 20.312 (20.810)	Acc@5 74.609 (76.669)
Epoch: [2][20/196]	Time 0.188 (0.218)	Data 0.002 (0.032)	Loss 2.0472 (2.0349)	Acc@1 17.188 (20.815)	Acc@5 79.688 (77.102)
Epoch: [2][30/196]	Time 0.187 (0.208)	Data 0.002 (0.022)	Loss 2.0059 (2.0330)	Acc@1 23.828 (21.384)	Acc@5 78.125 (77.571)
Epoch: [2][40/196]	Time 0.188 (0.203)	Data 0.002 (0.017)	Loss 2.0171 (2.0286)	Acc@1 24.609 (21.713)	Acc@5 79.297 (77.772)
Epoch: [2][50/196]	Time 0.187 (0.200)	Data 0.002 (0.014)	Loss 2.1010 (2.0238)	Acc@1 21.875 (22.120)	Acc@5 75.781 (77.964)
Epoch: [2][60/196]	Time 0.188 (0.198)	Data 0.002 (0.012)	Loss 2.0345 (2.0203)	Acc@1 20.703 (22.285)	Acc@5 75.000 (78.119)
Epoch: [2][70/196]	Time 0.188 (0.197)	Data 0.002 (0.011)	Loss 1.9333 (2.0157)	Acc@1 31.641 (22.519)	Acc@5 82.031 (78.301)
Epoch: [2][80/196]	Time 0.188 (0.195)	Data 0.002 (0.010)	Loss 1.8943 (2.0080)	Acc@1 24.219 (22.729)	Acc@5 82.812 (78.583)
Epoch: [2][90/196]	Time 0.188 (0.195)	Data 0.002 (0.009)	Loss 2.0605 (2.0021)	Acc@1 24.609 (22.931)	Acc@5 78.906 (78.880)
Epoch: [2][100/196]	Time 0.188 (0.194)	Data 0.002 (0.008)	Loss 1.9653 (1.9945)	Acc@1 24.609 (23.225)	Acc@5 82.812 (79.297)
Epoch: [2][110/196]	Time 0.188 (0.193)	Data 0.002 (0.008)	Loss 1.9103 (1.9917)	Acc@1 26.172 (23.332)	Acc@5 80.078 (79.304)
Epoch: [2][120/196]	Time 0.188 (0.193)	Data 0.002 (0.007)	Loss 1.8755 (1.9873)	Acc@1 27.734 (23.589)	Acc@5 87.109 (79.423)
Epoch: [2][130/196]	Time 0.188 (0.192)	Data 0.002 (0.007)	Loss 1.9150 (1.9823)	Acc@1 23.828 (23.718)	Acc@5 82.031 (79.604)
Epoch: [2][140/196]	Time 0.188 (0.192)	Data 0.002 (0.006)	Loss 1.8564 (1.9774)	Acc@1 27.734 (23.978)	Acc@5 84.375 (79.710)
Epoch: [2][150/196]	Time 0.187 (0.192)	Data 0.002 (0.006)	Loss 1.8608 (1.9702)	Acc@1 32.812 (24.296)	Acc@5 87.109 (80.029)
Epoch: [2][160/196]	Time 0.188 (0.192)	Data 0.002 (0.006)	Loss 1.8319 (1.9670)	Acc@1 29.688 (24.510)	Acc@5 83.984 (80.134)
Epoch: [2][170/196]	Time 0.188 (0.191)	Data 0.002 (0.006)	Loss 1.8389 (1.9632)	Acc@1 30.078 (24.735)	Acc@5 84.766 (80.238)
Epoch: [2][180/196]	Time 0.188 (0.191)	Data 0.002 (0.005)	Loss 1.8554 (1.9559)	Acc@1 28.516 (25.114)	Acc@5 86.719 (80.477)
Epoch: [2][190/196]	Time 0.188 (0.191)	Data 0.002 (0.005)	Loss 1.8667 (1.9519)	Acc@1 31.250 (25.327)	Acc@5 82.812 (80.604)
Test: [0/40]	Time 0.607 (0.607)	Data 0.550 (0.550)	Loss 1.8649 (1.8649)	Acc@1 29.297 (29.297)	Acc@5 82.812 (82.812)
Test: [10/40]	Time 0.056 (0.107)	Data 0.001 (0.052)	Loss 1.8862 (1.8710)	Acc@1 27.344 (30.575)	Acc@5 81.641 (83.168)
Test: [20/40]	Time 0.057 (0.083)	Data 0.001 (0.028)	Loss 1.9336 (1.8634)	Acc@1 25.000 (29.967)	Acc@5 83.594 (83.854)
Test: [30/40]	Time 0.057 (0.075)	Data 0.002 (0.019)	Loss 1.8163 (1.8670)	Acc@1 31.641 (30.003)	Acc@5 85.938 (83.770)
Saving at epoch:  3
slurmstepd: error: *** JOB 12809752 ON gr016 CANCELLED AT 2021-12-05T20:07:38 ***
